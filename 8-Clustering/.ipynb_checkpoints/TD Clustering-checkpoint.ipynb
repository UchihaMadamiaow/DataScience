{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Apprentissage non supervisé "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans les notebooks précédents, nous avons traité des problème de classification ou de régression supervisés. \n",
    "\n",
    "L'expérience sur laquelle nous nous basons pour construire des fonctions capables de faire des prédictions est cet ensemble de données décrites par un ensemble d'attributs et d'une cible (data + target).\n",
    "\n",
    "En classification non supervisée, seules les données sont disponibles, sans les étiquettes et souvent on ne connait même pas l'ensemble des étiquettes possibles. Les tâches d'apprentissage sont par exemple d'estimer leur distribution, de les représenter dans des espaces de plus petite dimension pour essayer de les \"comprendre\", ou encore de tenter de les regrouper en groupes homogènes.\n",
    "\n",
    "Par exemples :\n",
    "\n",
    "1. on peut faire l'hypothèse que la distribution des données est réalisée selon une loi normale (facile) ou un mélange de telles lois (beaucoup plus difficile), et il s'agit alors d'identifier les paramètres de ces lois. (Voir les [Mélanges Gaussiens](https://scikit-learn.org/stable/modules/mixture.html))\n",
    "2. on peut faire l'hypothèse que les données \"vivent\" dans un sous espace de bien plus faible dimension et que cet espace est localement euclidien. On cherche alors la transformation qui permet de passer de l'espace d'origine à cet espace réduit. Voir l'[apprentissage de variétés](https://scikit-learn.org/stable/modules/manifold.html)\n",
    "3. On peut faire l'hypothèse que les données sont réparties en groupes distincts mais homogènes et trouver des représentant de ces groupes. On réalise alors du clustering...\n",
    "\n",
    "Il existe de nombreuses autres approches pour l'apprentissage non supervisé. Mais derrière ces tâches, il s'agit encore  très souvent  d'optimiser une fonction de coût. Illustrons cela sur le clustering, qui cherche à regrouper des données en groupes homogènes. La fonction de coût va traduire cette homogénéité. Par exemple on peut avoir comme objectif de minimiser la somme des distances entre les points à l'intérieur de chaque groupe et maximiser les distances des points entre deux groupes. \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Clustering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Commencer par générer des données pour illustrer le clustering. On peut réutiliser la fonction `make_blobs` de scikit-learn.\n",
    "\n",
    "**Question** Générer un jeu de données dans $R^2$ avec `make_blobs` composé de 3 groupes de N=500 points centrés à des positions  différentes et dont l'écart type est de 0.5. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 500 # nombre de points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question** Tracer ces points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question** Modifier un peu les paramètres de votre génération de façon à voir 3 groupes, mais pas complètement séparés.  Les centres doivent être en (2, 1), (-1, -2) et  (1, -1). Appelez ces données `X_blobs`, `y_blobs`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Remarque** Vous pouvez choisir la couleur des points en indiquant `c` et `cmap` à l'appel de `scatter`et choisissez la carte des couleurs en regardant `plt.colormaps?`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On va réaliser maintenant un clustering avec [Kmeans](https://scikit-learn.org/stable/modules/generated/sklearn.cluster.KMeans.html#sklearn.cluster.KMeans). Dans cet algorithme, ([des années 50](https://en.wikipedia.org/wiki/K-means_clustering#History)), on va chercher $k$ points appelés centroïdes tels que la somme des distances des points du jeu de données à son centroïde le plus proche soit minimale. \n",
    "\n",
    "De façon formelle\n",
    "\n",
    "$$\\mathop{\\mathrm{argmin}}_{c_1,\\dots,c_k}\\sum_{x\\in X}\\bigg(\\min_{i\\in[1,k]}\\Vert x-c_i \\Vert^2\\bigg)$$\n",
    "\n",
    "Le problème est difficile. On peut considérer qu'il y a de fortes chances qu'il n'existe pas d'algorithme en temps polynomial pour le résoudre. ([Voir](https://www.sciencedirect.com/science/article/pii/S0304397510003269)) \n",
    "\n",
    "L'algorithme itère les étapes suivantes: \n",
    "\n",
    "1. Choisir $k$ points du jeu de données et les désigner comme centroïdes\n",
    "2. Tant qu'on n'a pas convergé:\n",
    "    1. Affecter chaque point à son centroïde le plus proche, constituant au plus $k$ groupes\n",
    "    2. Calculer les barycentres de chaque groupe. C'est le prochain ensemble de centroïdes\n",
    "    \n",
    "L'algorithme converge en fait en temps polynomial, mais l'étape 1 non déterministe, conduit à des solutions qui peuvent être différentes et on trouve alors une solution qui est un minimum local. On peut donc être amené à réitérer l'algorithme plusieurs fois pour tenter de trouver une \"bonne\" solution. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question** utiliser KMeans pour trouver les clusters et afficher les centroïdes sur le graphique.\n",
    "\n",
    "![Blobs avec leur centroides](blobsCentroides.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question** Vous pouvez modifier certains paramètres pour voir les effets. Modifiez notamment le nombre de clusters et affichez les résultats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question** Pourquoi existe-t-il un paramètre `n_init`?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Réponse*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-means à la main..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question** Définir un nombre de clusters à 3. Sélectionnez dans `id_centroids` trois indices dans l'ensemble de points `X_blobs`. Appelez `centroids` ces trois points.\n",
    "\n",
    "(*Aide* On utilise `np.random.choice`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question** Ah ah! Revenez sur le TD Broadcast pour revoir la méthode qui permet de calculer efficacement la distance au carré de tous les points à ces centroïdes dans une matrice `dist`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question** Calculez maintenant `pproche` qui est le résultat de `argmin` sur la matrice des distances. Tracez les points selon une couleur pour indiquer leur centroïde le plus proche.\n",
    "\n",
    "![Etape 1 de Kmeans](etape1Kmeans.png)\n",
    "\n",
    "*NB* le dessin va varier selon vos tirages!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question** Le centroïde le plus proche définit donc 3 groupes. Calculer le barycentre de chacun de ces groupes. Cela donne le nd-array appelé `new_centroids`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On a réalisé une étape de cet algorithme itératif. Pour savoir si on doit s'arrêter, une façon est de voir si les centroïdes ont bougé ou pas. On peut aussi accepter une certaine tolérance et regarder si les centroïdes ont bougé plus qu'un certain seuil.\n",
    "\n",
    "**Question** Calculer la somme des differences au carré entre les nouveaux et les anciens centroides."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question** En déduire un algorithme itératif implanté dans une fonction `my_kmeans`  qui prend en argument, les données, le nombre de clusters, la tolérance pour s'arrêter et un paramètre de verbosité. Si on veut être verbeux, on affichera à chaque  tour la nouvelle différence entre les anciens et nouveaux centroïdes. La fonction retourne les centroïdes et les affectations aux clusters (sous la forme d'un vecteur comme le vecteur de prédiction dans KMeans). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question** Faites fonctionner votre algorithme! et affichez vos résultats graphiquement!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Un peu de difficulté pour k-means"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question** Générer avec `make_moons` N exemples avec un bruit de 0.05 pour obtenir à peu près ceci. Appelez les données `X_moons` et `y_moons`.\n",
    "\n",
    "![moons](moons.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question** Appliquer k-means et observer le résultat. Qu'en pensez-vous ? Donnez une justification du résultat obtenu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Réponse*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spectral Clustering "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dans l'espace de représentation des données, une bonne mesure de distance ou de similarité n'est peut être pas la distance euclidienne. Ou encore, la représentation des données n'est peut être pas la bonne. Trouver la bonne distance ou représentation à utiliser pour des algorithmes comme k-means ou k-plus proches voisins, ce n'est pas facile du tout. Le spectral clustering que nous regarderons plus en détail l'année prochaine fait un kmeans mais dans un autre espace de représentation après une transformation non linéaire."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pour changer la représentation des données, utilisons `spectral_embedding`. L'idée est de considérer un graphe des k-plus proches voisins, et la distance est comme le temps de parcours moyen dans le graphe pour aller d'un point à un autre. Ainsi on utilise la distance euclidienne uniquement que très localement. \n",
    "\n",
    "![localement euclidien](./moonsmanifold.png)\n",
    "\n",
    "Le graphe des plus proches voisins n'est pas symétrique. C'est un graphe non dirigé. Ici on le transforme en graphe dirigé pour construire la nouvelle représentation des données et la nouvelle distance. \n",
    "\n",
    "Le `spectral_embedding` est ensuite un calcul un peu compliqué à expliquer ici qui va grossièrement aboutir à un nouvel espace dans lequel la distance euclidienne correspond au temps de parcours moyen dans pour faire un chemin d'un noeud à un autre du graphe.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import kneighbors_graph\n",
    "\n",
    "nb_voisins = 50\n",
    "graphe_dirige = kneighbors_graph(X_moons, nb_voisins, include_self=True)\n",
    "graphe_non_dirige = 0.5 * (graphe_dirige + graphe_dirige.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import spectral_embedding\n",
    "X_moons_spec = spectral_embedding(graphe_non_dirige, n_components=2, drop_first=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question** Tracer les points dans ce nouvel espace avec leur étiquette réelle. Que constatez-vous ? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Réponse*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question** Réaliser un kmeans dans ce nouvel espace avec 2 clusters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Le spectral clustering est très proche de la réalisation d'un spectral embedding suivi d'un K-means. \n",
    "\n",
    "**Question** Appliquer `SpectralClustering` sur les deux jeux de données. Et affichez graphiquement les résultats."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quel nombre de clusters ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Question** La recherche du nombre de clusters est complexe, surtout en l'absence de supervision... Quelques méthodes tentent de deviner le nombre de clusters. Essayez `DBSCAN` et `AffinityPropagation` d'abord sur les blobs et ensuite sur les moons.  (Mettez le paramètre `eps=.15` pour `DBSCAN`). Tracez les résultats "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
